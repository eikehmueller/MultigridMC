\documentclass[11pt]{article}
\usepackage[margin=2cm]{geometry}
\usepackage{amssymb,amsmath}
\usepackage{algorithm,algpseudocode}
\newcommand{\xtrue}{x_{\text{true}}}

\title{Smoothing and sampling with low-rank updates}
\author{Eike Mueller, University of Bath}

\begin{document}
\maketitle
Consider the following linear problem:
\begin{equation}
    Ax = \left(A_0 + B \Sigma^{-1} B^T \right)x = b\label{eqn:linear_system}
\end{equation}
Here $x,b\in\mathbb{R}^n$, $A,A_0$ are symmetric positive definite $n\times n$ matrices, where it is further assumed that $A_0$ is sparse. $\Sigma$ is a dense $m\times m$ matrix with $m\ll n$, and $B$ is an $n\times m$ matrix, so that $B\Sigma^{-1}B^T$ is a low-rank correction to $A_0$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Iterative method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Let $A_0 = M_0-N_0$, so that without the low-rank correction one can solve $A_0x=b$ with the iteration $x^{k+1} = M_0^{-1}(b + N_0x^k)$; for example, setting $M_0 = L_0 + D_0$ to the lower-triangular part of $A_0$ would lead to the familiar Gauss-Seidel iteration.

To solve Eq. \eqref{eqn:linear_system}, we now consider the splitting $A=M-N$ with $M=M_0 + B^T \Sigma^{-1} B$, leading to the iteration
\begin{equation}
    \left(M_0 + B \Sigma^{-1} B^T \right)x^{k+1} = b + N_0 x^k\label{eqn:low_rank_iteration}
\end{equation}
The matrix $M_0 + B \Sigma^{-1}B^T$ can be inverted with the Woodbury matrix identity
\begin{equation}
    \left(M_0 + B \Sigma^{-1} B^T \right)^{-1} = M_0^{-1} - M_0^{-1} B \left(\Sigma + B^TM_0^{-1}B\right)^{-1}B^TM_0^{-1},
\end{equation}
which leads to:
\begin{equation}
    x^{k+1} = \left(M_0^{-1}-M_0^{-1}B \overline{\Sigma}^{-1}B^T M_0^{-1}\right)\left(b+N_0x^k\right)
    \qquad\text{with $\overline{\Sigma} = \Sigma + B^TM_0^{-1}B$}.
\end{equation}
This can be written more compactly by introducing the $n\times m$ matrix $\overline{B} = M_0^{-1} B \overline{\Sigma}^{-1}$ and an intermediate variable $x^{k+1/2}$ as:
\begin{equation}
    \begin{aligned}
        x^{k+1/2} & = M_0^{-1}(b+N_0x^k) = x^k + M_0^{-1}(b-A_0x^k)                                                           \\
        x^{k+1}   & = \left(\text{Id}-\overline{B} B^T\right) x^{k+1/2} = x^{k+1/2} - \overline{B} \left(B^T x^{k+1/2}\right)
    \end{aligned}
\end{equation}
Note that $x^{k+1/2}$ can be computed with the standard iterative method defined by the splitting $A_0=M_0-N_0$ applied to the equation $A_0x=b$. The entire method is written down in detail in Alg. \ref{alg:low_rank_stationary}.
\begin{algorithm}
    \caption{Stationary iteration with low-rank update}\label{alg:low_rank_stationary}
    \begin{algorithmic}[1]
        \State Compute the dense $m\times m$ matrix $\overline{\Sigma}^{-1} = (\Sigma+B^TM_0^{-1}B)^{-1}$
        \State Compute the $n\times m$ matrix $\overline{B}= M_0^{-1} B\overline{\Sigma}^{-1}$
        \State Choose some initial $x^0$
        \For {$k=0,1,\dots,K_{\max}-1$}
        \State Compute $x^{k+1/2} = M_0^{-1} (b+N_0 x^k)$ as usual
        \State Compute the $m$-dimensional vector $y = B^T x^{k+1/2}$
        \State Set $x^{k+1} = x^{k+1/2} - \overline{B} y$
        \EndFor
    \end{algorithmic}
\end{algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Convergence rate}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Observe that the true solution $\xtrue$ satisfies
\begin{equation}
    \begin{aligned}
        b & = (X_0 + B\Sigma^{-1}B^T)\xtrue               \\
          & = (M_0 + B\Sigma^{-1}B^T) \xtrue - N_0 \xtrue
    \end{aligned}
\end{equation}
Inserting this into Eq. \eqref{eqn:low_rank_iteration} leads to the following relationship for the error $e^k:= x^k-\xtrue$:
\begin{equation}
    (M_0 + B\Sigma^{-1}B^T)e^{k+1} = N_0 e^k
\end{equation}
Or, equivalently:
\begin{equation}
    e^{k+1} = \left(\text{Id} + M_0^{-1}B\Sigma^{-1}B^T \right)^{-1} \left(\text{Id}-M_0^{-1}A_0\right) e^k
\end{equation}
Note that the factor $\text{Id} - M_0^{-1}A_0$ arises in the stationary iteration with splitting $A_0=M_0-N_0$ for the system $A_0x=b$. Further, if $M_0$ is positive definite then $M_0^{-1}B\Sigma^{-1}B^T $ is positive definite and the factor $\left(\text{Id} + M_0^{-1}B\Sigma^{-1}B^T \right)^{-1}$ is contracting, i.e.
\begin{equation}
    ||\left(\text{Id} + M_0^{-1}B\Sigma^{-1}B^T \right)^{-1}v|| \le ||v||
\end{equation}
Hence, if the original method with contracting factor $\text{Id} - M_0^{-1}A_0$ converges, then the smoother with the low-rank update will also converge.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sampling}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
As stated in Colin's paper \cite{Fox2017}, given a decomposition $A=M-N$, we can draw turn the smoother into a sampler as shown in Alg. \ref{alg:general_sampling} (which is \cite[Algorithm 5]{Fox2017}):
\begin{algorithm}
    \caption{Sampling from $\mathcal{N}(A^{-1},A^{-1}f)$ with matrix splitting $A=M-N$}\label{alg:general_sampling}
    \begin{algorithmic}[1]
        \State Pick $x^0$
        \For {$k=0,1,\dots$}
        \State Draw $c^k\sim\mathcal{N}(f,M^T+N)$
        \State Set $x^{k+1} = x^{k} + M^{-1}\left(c^k - Ax^k \right)$
        \EndFor
    \end{algorithmic}
\end{algorithm}
For $k\rightarrow \infty$ the distribution of the $x^k$ converges to $\mathcal{N}(A^{-1}f,A^{-1})$. If $x^0\sim\mathcal{N}(A^{-1}f,A^{-1})$, then $x^k\sim \mathcal{N}(A^{-1}f,A^{-1})$ for all $k$.

If we use an (overrelaxed) Gibbs-sampler, i.e. $M_0=\omega^{-1} D+L$ (or $M=\omega^{-1} D+L^T$ for the equivalent backward iteration), then we have that
\begin{equation}
    M^T+N = \Sigma_{\text{SOR}} = \frac{2-\omega}{\omega}D + B\Sigma^{-1}B^T,
\end{equation}
in other words, in Alg. \ref{alg:general_sampling} we have to be able to sample from multivariate normal distribution $\mathcal{N}(\Sigma_{\text{SOR}},f)$ with mean $f$ and a covariance matrix $\Sigma_{\text{SOR}}=\frac{2-\omega}{\omega}D + B\Sigma^{-1}B^T$ that is diagonal plus a low-rank update. To achieve this, draw $\xi^k_\text{diag}\sim\mathcal{N}(0,I_{n\times n})\in\mathbb{R}^n$ and $\xi^k_\text{LR}\sim\mathcal{N}(0,I_{m\times m})\in\mathbb{R}^m$ and set
\begin{equation}
    c^k = \sqrt{\frac{2-\omega}{\omega}}D^{1/2} \xi^k_{\text{diag}} + B\Sigma^{-1/2} \xi^k_{\text{LR}} + f.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallelisation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
The SOR smoother is inherently sequential, so it can not be used in a parallel context. To address this issue, we can use the red-black (RB) reordered variant. For this, we write
\begin{equation}
    x = P x_{\text{rb}} \qquad \text{with $x_{rb} = \begin{pmatrix}x_r \\ x_b\end{pmatrix}$}.
\end{equation}
Here $P$ is a permutation matrix. The vectors $x_r$ and $x_b$ contain the unknowns at the ``red'' and ``black'' lattice sites. Then, after multiplication by $P$, $A_0x=b$ is equivalent to
\begin{equation}
    (PA_0P^T) Px = Px \quad \Leftrightarrow\quad A_{0,rb} x_{rb} = b_{rb}
\end{equation}
We can write $A_{0,rb}$ as
\begin{equation}
    A_{0,rb} = \begin{pmatrix}
        D_{rr} & U_{rb} \\
        L_{rb} & D_{bb}
    \end{pmatrix}
\end{equation}
where $D_{rr}$ and $D_{bb}$ are diagonal matrices. Since $A_0$ is symmetric, the matrix $A_{0,rb} = PA_0P^T$ is also symmetric and hence $U_{rb} = L_{rb}^T$.
The forward RB SOR smoother corresponds to the matrix splitting
\begin{xalignat}{2}
    M_{0,rb} &= \begin{pmatrix}
        \frac{1}{\omega}D_{rr} & 0                      \\
        L_{rb}                 & \frac{1}{\omega}D_{bb}
    \end{pmatrix},&
    N_{0,rb} = M_{0,rb} - A_{rb} &= \begin{pmatrix}
        \frac{1-\omega}{\omega}D_{rr} & U_{rb}                        \\
        0                             & \frac{1-\omega}{\omega}D_{bb}
    \end{pmatrix}
\end{xalignat}
Note that in the corresponding sampler we still need to draw from a distribution with diagonal covariance matrix since
\begin{equation}
    M_{0,rb}^T + N_{0,rb} = \begin{pmatrix}
        \frac{2-\omega}{\omega}D_{rr} & 0                             \\
        0                             & \frac{2-\omega}{\omega}D_{bb}
    \end{pmatrix}
\end{equation}
Hence in practice nothing changes, we can use the same sampling algorithm as above but iterate over the unknowns in a different order in the SOR sweep.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{unsrt}
\bibliography{low_rank_smoother}
\end{document}
